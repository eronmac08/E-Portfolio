[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Geomatics Portfolio",
    "section": "",
    "text": "About Me\nHello,\nI am a Master’s student in Geomatics for Environmental Management at the University of British Columbia (UBC) in Vancouver, BC, Canada, with an expected graduation date of April 5th, 2025. I also hold a B.Sc. in Environmental Science with a focus on Marine Ecology from Western Washington University in Bellingham, WA.\nAfter my undergraduate studies, I spent four years contributing to salmon stock assessments through collaborations with both U.S. Fish and Wildlife and the Washington Department of Fish and Wildlife on the West Coast of Washington State. This hands-on experience helped me build a strong foundation in geospatial analysis, biological sciences and environmental research.\nCurrently, I am eager to transition into a career that blends my geomatics expertise with my passion for environmental management. I am particularly focused on exploring the the world of Geographic Information Systems (GIS) and their applications in addressing pressing environmental challenges.\n\n\nSkills\n\nProcient in GIS software: ArcGIS Pro, QGIS\nProgramming languages: Python, R\nSpatial data analysis and manipulation\nRemote sensing and cartographic design\nDatabase management: SQL, Microsoft Acces"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "&lt;iframe src=\"./E-Portfolio/Resume\" width=\"100%\" height=\"800px\"&gt;&lt;/iframe&gt; \n\nEducation\n\nUniversity of British Columbia - Master of Geomatics for Environmental Management | April, 2025\nAdvanced Geographic Information Systems for Environmental Management Remote Sensing for Ecosystem Management Advanced Earth Observation and Image Processing Geospatial Data Analysis Spatial Statistics Landscape Ecology and Management\nBachelor of Science in Marine Ecology\n- College of the Environment, Western Washington University | December, 2018\n\n\n\nExperience\n\nBiological Science Technician 2, Full Time $25(US)/HR\nWashington Department of Fish and Wildlife - Bellingham, WA. | March 2023-August 2024\nOwen Donohoe (978) 766-7882, Owen.Donohoe@dfw.wa.gov\n\nCreated potential spawning ground habitat maps using ArcGIS Pro for areas not currently surveyed using existing spawning ground data.\nConducted comprehensive salmon and steelhead spawning ground, fish abundance and presence surveys by walking or rafting streams and rivers\nIdentified, counted and marked over 1,500 redds using ArcGIS Pro, R and excel to create spatial maps representing spawning ground locations for seven species of fish\nUtilized maps and graphs created with R and ArcGIS to determine current and future salmon/steelhead runs in order to set local recreational fishing seasons and regulations\nCollected biological samples from live and dead salmonids including species type, DNA, scales, lengths, weights, sex, otoliths and CWT’s\n\nBiological Science Technician 2, Full Time $22(US)/HR\nWashington Department of Fish and Wildlife - La Conner, WA. | March 2022-February 2023\nJustin Spinelli (425) 739-1024, Justin.Spinelli@dfw.wa.gov\n\nConducted standard lake surveys via boat electrofishing, gillnets and fyke nets on washington lakes to determine biodiversity\nIndependently worked on field research in diverse and unpredictable terrain and conditions \nTransported, floy tagged and released mature rainbow trout for recreational fishing\nIndependently maintained, trailered, launched and operated boats 19ft. and 26ft long to conduct creel surveys on local lakes\nIdentified and biosampled warm water species including walleye, kokanee, rainbow, browntrout, cutthroat, yellow perch, burbot, whitefish, bullhead, crappie, rockbass, bass including sex, length, marks, scales, GSI, otoliths, and genetic samples\nExtracted stomach samples from yellow perch, rock bass and walleye to determine salmonid predation rates\nConducted creel surveys to obtain efficacy of liberalized regulations\nCaptured white crappie vie fyke nets and transported to reintroduce self-sustaining population for recreational fishery \nTransported and Spawned adult chum salmon\n\nBiological Science Technician 2 - Smolt Trapping, Full Time $22(US)/HR\nWashington Department of Fish and Wildlife - Twisp, WA. | November 2021-December 2021\nNathaniel Fuchs (760) 660-8120, Nathaniel.Fuchs@dfw.wa.gov\n\nDetermine overall out-migration of salmonid smolt populations to calculate mortality rates associated with dams to predict future adult returns\nCaptured, identified, biosampled and PIT-tagged juvenile/adult ESA-listed salmonids and fish species in North Central Washington rivers via rotary screw trap\nAssisted with in-stream installation, removal, and repair of juvenile screw trap\nMonitored and operated screw trap based on real-time observations and forecasted environmental conditions, including flood events\nEntered, managed and uploaded juvenile PIT tagging files into P4 to conduct queries in PTAGIS and Access databases\nPerformed quality assurance and quality control checks on collected data \n\nBiological Science Technician 2 - Creeler, Full Time $22(US)/HR\nWashington Department of Fish and Wildlife - Twisp, WA. | August 2021-October 2021\nRyan Fortier (509) 720-6424, Ryan.Fortier@dfw.wa.gov\n\nBiosampled summer Chinook including sex, length, marks, scales, and genetic samples\nDissected and read coded wire tags collected from Columbia River creel surveys to determine origin of fish\nConducted creel surveys to obtain catch number and effort at Columbia and Methow River fishing locations\nAccurately recorded and entered all data associated with surveys of the fishery\nIdentified freshwater and anadromous fish species in eastern Washington caught in screw trap, including but not limited to spring/summer Chinook, rainbow/steelhead trout, Coho, Sockeye, whitefish, Bull trout, Westslope cutthroat, etc\nAided in lake rehabilitation including the application of rotenone to eliminate invasive species\n\nBiological Science Technician, Full Time $14(US)/HR\nUS Fish and Wildlife Service - Winthrop, WA. | March 2021-June 2021\nMichael Humling (509) 423-0533, Michael.Humling@usda.gov\n\nCreated fish presence maps via ArcGIS to aid in collection of steelhead broodstock\nIdentified, PIT-tagged, Floy-tagged, and biosampled adult steelhead trout\nCollected adult steelhead broodstock via weir traps, hatchery outfalls, and angling\nConducted steelhead spawning ground surveys\nSampled juvenile salmonids via backpack electrofishing with the Yakama tribe.\nPerformed gonadosomatic index (GSI) sampling of hatchery spring Chinook and steelhead after volitional releases to determine sexual maturity\nGained extensive knowledge of P4, PTAGIS, HPR PIT Tag readers and CWT detectors\nAided in live-spawning of steelhead and transfer to the kelt reconditioning program\n\nCommercial Salmon Fisherman - Deck Boss / Relief Captain, Full Time 30K(US)/YR\nSnag Fisheries/Silver Bay Seafoods - Bristol Bay, AK | June-August 2005-2021\nEric Marble (360) 391-4155, Eric.Marble@ymail.com\n\nCommunicated with biologists to create an interface between fishermen and scientists\nEnforced sustainable regulations within crew\nRan and performed maintenance on hydraulic, electrical, and mechanical machinery as well as repairing fishing nets\nManaged a crew in a fast paced and unpredictable environment\nPlanned daily fishing operations by identifying the best fishing grounds and assessing the current weather, tides and sea conditions\nRecruited, evaluated and hired crewmen for fishing seasons\n\nCommercial Crabber -Deck Boss, Full Time 10K(US)/YR\nWhitlock and Sons Fisheries INC. - Puget Sound/Salish Sea, WA. |  October-April 2016-2021\nSpencer Whitlock (425) 530-5953, Pugetsoundcrab@gmail.com\n\nAdvocated with local board of fisheries and tribal members for a fair and sustainable quota\nProficient with navigational equipment\nAnalyzed weather and habitat conditions to create maps predicting crab movement and presence\nIdentified and measured catch to determine whether they meet legal size requirements and released all bycatch\nWorked as part of an efficient team in close quarters for extended periods of time\n\n\n\n\nAwards & Honors\n\nBENJAMIN A. GILMAN INTERNATIONAL SCHOLARSHIP\nIIE GENERATION STUDY ABROAD SCHOLARSHIP\nDEAN’S HONORS LIST\n\n\n\nTeaching Assistantships\n\nList any TA positions you may have held.\n\n\n\nInstitutional Service\n\nVolunteering etc."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Contact",
    "section": "",
    "text": "Contact Me\nIf you have any questions or would like to discuss opportunities, feel free to reach out using the form below.\n\n\nHTML for the contact form\n\nYour Name: \nYour Email: \nYour Message:"
  },
  {
    "objectID": "content_development.html",
    "href": "content_development.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Project Deliverable 1\nThis is a sample page where you can archive project deliverables."
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Project: LiDAR-Based Individual Tree Segmentation\nIn this project, I used LiDAR data to segment individual trees from a random plot within a forest. The goal was to apply point cloud and Canopy Height Model (CHM)-based methods for tree detection and segmentation. The process involved extracting a plot, filtering outliers, normalizing the data, and performing tree segmentation using algorithms like li2012 and dalponte2016.\nSkills & Tools Used:\n\nR Programming: Utilized R for data manipulation, analysis, and visualization.\nLiDAR Processing: Used the lidR package to load, filter, and normalize LiDAR data, removing outliers and correcting for ground elevation.\nTree Segmentation: Applied the li2012 algorithm on the point cloud data for tree segmentation and used the dalponte2016 algorithm on CHM data to detect tree tops and segment individual trees.\nVisualization: Created 3D visualizations of segmented trees using the rgl package and compared results from both methods.\nRaster Analysis: Generated Canopy Height Models (CHM) at 0.5m resolution using terra to improve segmentation accuracy.\n\nResults:\nThe segmentation results showed that both methods identified individual trees with high accuracy. The li2012 algorithm effectively segmented trees based on the point cloud, while the dalponte2016 method produced accurate tree crown delineation using the CHM. The visualizations confirmed that tree segmentation was successful, providing clear distinctions between individual trees.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this project, I modeled riparian forest management within the Nahmint watershed, British Columbia, using a Digital Elevation Model (DEM) to define riparian reserve zones and management areas. The objective was to apply best management practices (BMPs) for forest managers to designate non-harvestable and harvestable zones near stream networks, assessing stream networks and classifying streams to inform land management decisions.\n\n\n\nArcGIS Pro & QGIS: Proficiently used both platforms to analyze DEMs, calculate flow direction, stream order, and derive hydrological insights using raster and vector tools.\nHydrological Modeling: Employed tools like “Flow Direction,” “Flow Accumulation,” and “Watershed” to identify stream networks and delineate watersheds, using thresholds to select relevant streams.\nStream Classification: Applied the Strahler and Shreve stream ordering methods to classify streams by their complexity and tributary structure, helping define riparian zones.\nPython Scripting: Used Python to automate field calculations for stream width, gradient, and buffer distances for riparian zone classification.\nBuffer Analysis: Created buffer zones around stream networks for Riparian Management Areas (RMAs), ensuring protection through customized reserve and management zone buffer distances.\n\n\n\n\nThe project identified key areas for Riparian Reserve Zones (RRMZ) and Riparian Management Zones (RMMZ) by analyzing stream class, gradient, and width. The analysis revealed areas with varying levels of protection, emphasizing the importance of buffer zones around fish-bearing streams. The results showed that stream networks in the Nahmint watershed were well classified, with over 80% of streams being fish-bearing and requiring buffer zones that varied from 10m to 100m depending on stream class.\n\n\n\n\n\n\nSample code snippet. Notice that you can provide a toggle to switch between coding languages - this is referred to as a ‘tabset’ in quarto. It is good practice to try and convert your R code to python, and vice-versa to demonstrate coding proficiency. For example, let’s showcase a function for calculating NDVI in R and Python.\n\nRPython\n\n\ncalc_ndvi &lt;- function(nir, red){ ndvi &lt;- (nir-red)/(nir+red) return(ndvi) }\n\n\ndef calc_ndvi(nir, red): \n  ndvi = (nir.astype(float)-red.astype(float))/(nir.astype(float)+red.astype(float))\n  return(ndvi)\n\n\n\n\n\n\n\n\nIn this project, I conducted a Least Cost Path (LCP) analysis to model Grizzly Bear movement across the Yellowhead region, using various environmental and human factors to assess movement costs. The goal was to identify the most efficient route between two points in the landscape.\n\n\n\nQGIS & ArcGIS Pro: Utilized both QGIS for raster manipulation and ArcGIS Pro for the final LCP analysis, showcasing proficiency in both platforms.\nRaster Analysis: Used tools like “Slope,” “Reclassify,” and “Raster Calculator” in QGIS to derive cost surfaces based on slope, land cover, and road proximity.\nReclassification & Weighting: Reclassified land cover data to assign resistance values and weighted layers to combine them into a unified cost surface.\nProximity Analysis: Created distance-to-roads rasters and applied cost models to factor in human infrastructure’s impact on bear movement.\nLeast Cost Path Analysis: Applied the “Distance Accumulation” and “Optimal Path as Line” tools in ArcGIS Pro to trace the most efficient path, integrating all cost factors.\n\n\n\n\nThe least cost path model revealed that Grizzly Bears would most likely avoid steep slopes and areas with high human infrastructure, favoring routes through forested and wetland areas. The weighted cost surface emphasized the avoidance of roads and areas with high land cover resistance, resulting in paths that were longer but more likely to offer safer movement corridors. This model can be used to inform conservation strategies and guide habitat protection efforts.\n\n\n\n\n\n\n\n\nThis GIS project focuses on conducting a suitability and overlay analysis to identify the best areas for expanding a humpback whale sanctuary in Hawaii. The goal is to find locations that balance the whales’ habitat needs with minimizing human impacts, particularly from vessel traffic.\n\n\n\nArcGIS Pro: Utilized for geospatial data analysis, including raster calculations, spatial analysis, and map production.\nSpatial Analyst Tools: Used for overlay analysis to determine suitable locations based on environmental and human impact factors.\nModelBuilder: Employed to create an automated workflow for the analysis process, improving efficiency and ensuring reproducibility.\nGeodatabase: Managed and organized the datasets, including Humpback_BIA and Humpback_Hawaii_BIA, ensuring proper data structures and cleaning unnecessary fields.\nSymbology and Cartography: Applied to visually represent the analysis results clearly and effectively in the final map and report.\n\n\n\n\n\nTop 5 Suitable Areas: The analysis identified the best five locations for expanding the whale sanctuary based on habitat suitability and minimizing human interference from vessel traffic.\nHumpback Whale Habitat Suitability: A detailed suitability map was created, combining various environmental factors and human impact data to highlight optimal sanctuary zones.\nFinal Map and Report: The final map clearly displayed the most suitable areas for the sanctuary expansion, and the report summarized the analysis steps, methodology, and the rationale for the selected areas."
  },
  {
    "objectID": "content.html#leaflet",
    "href": "content.html#leaflet",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Individual tree segmentation derived from a LiDAR point cloud using the Dalponte et al. (2016) algorithm in R (Verion 4.3.2)"
  },
  {
    "objectID": "content.html#code-snippets",
    "href": "content.html#code-snippets",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Sample code snippet. Notice that you can provide a toggle to switch between coding languages - this is referred to as a ‘tabset’ in quarto. It is good practice to try and convert your R code to python, and vice-versa to demonstrate coding proficiency. For example, let’s showcase a function for calculating NDVI in R and Python.\n\nRPython\n\n\ncalc_ndvi &lt;- function(nir, red){ ndvi &lt;- (nir-red)/(nir+red) return(ndvi) }\n\n\ndef calc_ndvi(nir, red): \n  ndvi = (nir.astype(float)-red.astype(float))/(nir.astype(float)+red.astype(float))\n  return(ndvi)"
  },
  {
    "objectID": "content.html#external-links",
    "href": "content.html#external-links",
    "title": "Content & Deliverables",
    "section": "",
    "text": "You can also provide a frame linking to external websites. For example, here is a link to a Google Earth Engine application I developed. The full-screen GEE application is available here in case you’re interested.\n(To use the GEE tool, navigate to any city you’d like, hit apply filters, and click anywhere on the map to retrieve a time-series of Landsat surface temperature observations for that point. Areas where the maximum temp exceeded 35 degrees Celsius in your date-range are highlighted in red.)"
  },
  {
    "objectID": "content.html#leafle",
    "href": "content.html#leafle",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Individual tree segmentation derived from a LiDAR point cloud using the Dalponte et al. (2016) algorithm in R (Verion 4.3.2)"
  },
  {
    "objectID": "content.html#nahmint-watershed.",
    "href": "content.html#nahmint-watershed.",
    "title": "Content & Deliverables",
    "section": "",
    "text": "In this project, I modeled riparian forest management within the Nahmint watershed, British Columbia, using a Digital Elevation Model (DEM) to define riparian reserve zones and management areas. The objective was to apply best management practices (BMPs) for forest managers to designate non-harvestable and harvestable zones near stream networks, assessing stream networks and classifying streams to inform land management decisions.\n\n\n\nArcGIS Pro & QGIS: Proficiently used both platforms to analyze DEMs, calculate flow direction, stream order, and derive hydrological insights using raster and vector tools.\nHydrological Modeling: Employed tools like “Flow Direction,” “Flow Accumulation,” and “Watershed” to identify stream networks and delineate watersheds, using thresholds to select relevant streams.\nStream Classification: Applied the Strahler and Shreve stream ordering methods to classify streams by their complexity and tributary structure, helping define riparian zones.\nPython Scripting: Used Python to automate field calculations for stream width, gradient, and buffer distances for riparian zone classification.\nBuffer Analysis: Created buffer zones around stream networks for Riparian Management Areas (RMAs), ensuring protection through customized reserve and management zone buffer distances.\n\n\n\n\nThe project identified key areas for Riparian Reserve Zones (RRMZ) and Riparian Management Zones (RMMZ) by analyzing stream class, gradient, and width. The analysis revealed areas with varying levels of protection, emphasizing the importance of buffer zones around fish-bearing streams. The results showed that stream networks in the Nahmint watershed were well classified, with over 80% of streams being fish-bearing and requiring buffer zones that varied from 10m to 100m depending on stream class."
  },
  {
    "objectID": "Contact.html",
    "href": "Contact.html",
    "title": "Contact",
    "section": "",
    "text": "Contact Me\nIf you have any questions or would like to discuss opportunities, feel free to reach out using the form below.\n\n\nHTML for the contact form\n\nYour Name: \nYour Email: \nYour Message:"
  },
  {
    "objectID": "Contact.html#contact-me",
    "href": "Contact.html#contact-me",
    "title": "Contact",
    "section": "",
    "text": "If you have any questions or would like to discuss opportunities, feel free to reach out using the form below.\n&lt;form action=“https://formspree.io/f/maykgnwa” method=“POST”&gt; &lt;label for=“name”&gt;Your Name:&lt;/label&gt;&lt;br&gt; &lt;input type=“text” id=“name” name=“name” required&gt;&lt;br&gt;&lt;br&gt;\n&lt;label for=“email”&gt;Your Email:&lt;/label&gt;&lt;br&gt; &lt;input type=“email” id=“email” name=“email” required&gt;&lt;br&gt;&lt;br&gt;\n&lt;label for=“message”&gt;Your Message:&lt;/label&gt;&lt;br&gt; &lt;textarea id=“message” name=“message” rows=“4” required&gt;&lt;/textarea&gt;&lt;br&gt;&lt;br&gt;\n&lt;input type=“submit” value=“Send Message”&gt; &lt;/form&gt; ](— title: “Contact” output: html_document —Contact MeIf you have any questions or would like to discuss opportunities, feel free to reach out using the form below.\n\nYour Name: Your Email: Your Message:\n\n\n\n)"
  },
  {
    "objectID": "Contact.html#section",
    "href": "Contact.html#section",
    "title": "Contact",
    "section": "```",
    "text": "```"
  },
  {
    "objectID": "Contact Me.html",
    "href": "Contact Me.html",
    "title": "Contact Me",
    "section": "",
    "text": "If you have any questions or would like to discuss opportunities, feel free to reach out using the form below.\n\nContact form\n\nYour Name: \nYour Email: \nYour Message:"
  },
  {
    "objectID": "content.html#dalponte-3d-map",
    "href": "content.html#dalponte-3d-map",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Individual tree segmentation derived from a LiDAR point cloud using the Dalponte et al. (2016) algorithm in R (Verion 4.3.2)"
  },
  {
    "objectID": "content.html#gif-treeessssss",
    "href": "content.html#gif-treeessssss",
    "title": "Content & Deliverables",
    "section": "GIF TREEESSSSSS",
    "text": "GIF TREEESSSSSS"
  },
  {
    "objectID": "content.html#least-cost-analysis",
    "href": "content.html#least-cost-analysis",
    "title": "Content & Deliverables",
    "section": "",
    "text": "In this project, I conducted a Least Cost Path (LCP) analysis to model Grizzly Bear movement across the Yellowhead region, using various environmental and human factors to assess movement costs. The goal was to identify the most efficient route between two points in the landscape.\n\n\n\nQGIS & ArcGIS Pro: Utilized both QGIS for raster manipulation and ArcGIS Pro for the final LCP analysis, showcasing proficiency in both platforms.\nRaster Analysis: Used tools like “Slope,” “Reclassify,” and “Raster Calculator” in QGIS to derive cost surfaces based on slope, land cover, and road proximity.\nReclassification & Weighting: Reclassified land cover data to assign resistance values and weighted layers to combine them into a unified cost surface.\nProximity Analysis: Created distance-to-roads rasters and applied cost models to factor in human infrastructure’s impact on bear movement.\nLeast Cost Path Analysis: Applied the “Distance Accumulation” and “Optimal Path as Line” tools in ArcGIS Pro to trace the most efficient path, integrating all cost factors.\n\n\n\n\nThe least cost path model revealed that Grizzly Bears would most likely avoid steep slopes and areas with high human infrastructure, favoring routes through forested and wetland areas. The weighted cost surface emphasized the avoidance of roads and areas with high land cover resistance, resulting in paths that were longer but more likely to offer safer movement corridors. This model can be used to inform conservation strategies and guide habitat protection efforts."
  },
  {
    "objectID": "content.html#recommended-whale-sanctuary",
    "href": "content.html#recommended-whale-sanctuary",
    "title": "Content & Deliverables",
    "section": "",
    "text": "This GIS project focuses on conducting a suitability and overlay analysis to identify the best areas for expanding a humpback whale sanctuary in Hawaii. The goal is to find locations that balance the whales’ habitat needs with minimizing human impacts, particularly from vessel traffic.\n\n\n\nArcGIS Pro: Utilized for geospatial data analysis, including raster calculations, spatial analysis, and map production.\nSpatial Analyst Tools: Used for overlay analysis to determine suitable locations based on environmental and human impact factors.\nModelBuilder: Employed to create an automated workflow for the analysis process, improving efficiency and ensuring reproducibility.\nGeodatabase: Managed and organized the datasets, including Humpback_BIA and Humpback_Hawaii_BIA, ensuring proper data structures and cleaning unnecessary fields.\nSymbology and Cartography: Applied to visually represent the analysis results clearly and effectively in the final map and report.\n\n\n\n\n\nTop 5 Suitable Areas: The analysis identified the best five locations for expanding the whale sanctuary based on habitat suitability and minimizing human interference from vessel traffic.\nHumpback Whale Habitat Suitability: A detailed suitability map was created, combining various environmental factors and human impact data to highlight optimal sanctuary zones.\nFinal Map and Report: The final map clearly displayed the most suitable areas for the sanctuary expansion, and the report summarized the analysis steps, methodology, and the rationale for the selected areas."
  },
  {
    "objectID": "content.html#tree-segmentation.",
    "href": "content.html#tree-segmentation.",
    "title": "Content & Deliverables",
    "section": "",
    "text": "Project: LiDAR-Based Individual Tree Segmentation\nIn this project, I used LiDAR data to segment individual trees from a random plot within a forest. The goal was to apply point cloud and Canopy Height Model (CHM)-based methods for tree detection and segmentation. The process involved extracting a plot, filtering outliers, normalizing the data, and performing tree segmentation using algorithms like li2012 and dalponte2016.\nSkills & Tools Used:\n\nR Programming: Utilized R for data manipulation, analysis, and visualization.\nLiDAR Processing: Used the lidR package to load, filter, and normalize LiDAR data, removing outliers and correcting for ground elevation.\nTree Segmentation: Applied the li2012 algorithm on the point cloud data for tree segmentation and used the dalponte2016 algorithm on CHM data to detect tree tops and segment individual trees.\nVisualization: Created 3D visualizations of segmented trees using the rgl package and compared results from both methods.\nRaster Analysis: Generated Canopy Height Models (CHM) at 0.5m resolution using terra to improve segmentation accuracy.\n\nResults:\nThe segmentation results showed that both methods identified individual trees with high accuracy. The li2012 algorithm effectively segmented trees based on the point cloud, while the dalponte2016 method produced accurate tree crown delineation using the CHM. The visualizations confirmed that tree segmentation was successful, providing clear distinctions between individual trees."
  }
]